import json
from datasets import Dataset
from dotenv import load_dotenv
load_dotenv()
from ragas import evaluate
from ragas.metrics import (
    # context_precision,   #  De momento los quitamos
    # context_recall,
    faithfulness,
    answer_relevancy,
)
from ragas.embeddings import HuggingfaceEmbeddings
from langchain_openai import OpenAIEmbeddings, OpenAI
from ragas.llms import llm_factory
llm = llm_factory("gpt-4.1", client=OpenAI())

embedding = OpenAIEmbeddings(model="text-embedding-ada-002")  
LOG_PATH = "rag_logs.jsonl"


def cargar_logs():
    datos = []
    with open(LOG_PATH, "r", encoding="utf-8") as f:
        for linea in f:
            item = json.loads(linea.strip())
            datos.append(
                {
                    "question": item["question"],
                    "answer": item["answer"],
                    "contexts": item["contexts"],
                }
            )
    return datos


def main():
    print(" Cargando registros de interacci贸n RAG desde:", LOG_PATH)
    registros = cargar_logs()

    if len(registros) == 0:
        print(" No se encontraron logs. Ejecuta el sistema RAG primero.")
        return

    ds = Dataset.from_list(registros)

    print("\nEjecutando evaluaci贸n con RAGAS...\n")

    #  De momento solo usamos m茅tricas que NO requieren 'reference'
    resultado = evaluate(
        dataset=ds,
        metrics=[faithfulness, answer_relevancy],
        embeddings=embedding,
    )

    print("\nResultados de la evaluaci贸n:")
    print(resultado)

    print("\nExplicaci贸n de las m茅tricas usadas ahora mismo:")
    print("""
    - faithfulness: Comprueba si la respuesta est谩 realmente apoyada en los 
      documentos. Un valor bajo indica alucinaciones o informaci贸n inventada.

    - answer_relevancy: Mide si la respuesta realmente aborda la pregunta. 
      Un valor bajo significa que la respuesta es irrelevante, incompleta o vaga.
    """)

    print("\nNota:")
    print("""
    Las m茅tricas 'context_precision' y 'context_recall' en esta versi贸n de RAGAS
    necesitan una columna adicional 'reference' (respuesta de referencia
    anotada a mano). Cuando tengas un conjunto de evaluaci贸n con respuestas
    de referencia, podremos a帽adir:

        - context_precision
        - context_recall

    y as铆 evaluar la calidad del recuperador de forma m谩s completa.
    """)

    print("\nEvaluaci贸n completada. Ajusta el recuperador (top-k, embeddings, "
          "tama帽o de chunk) y vuelve a ejecutar para comparar mejoras.")


if __name__ == "__main__":
    main()